{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[ 6.56036701  5.9023163   6.56072183  7.29      ]\n",
      " [ 7.28959382 -4.99999305  6.55551366  8.1       ]\n",
      " [ 8.09876089  9.          7.28908112  8.09627994]\n",
      " [ 6.56058959  0.35262341  2.7526826  -4.24952682]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 8.09769356 10.         -4.99410491  8.99125599]\n",
      " [ 2.02311726  0.          0.          0.        ]\n",
      " [-0.5         0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the grid-world environment as a 3x3 matrix\n",
    "# 'S' denotes the starting state\n",
    "# 'G' denotes the goal (terminal) state with a reward of 10\n",
    "# 'X' denotes an obstacle with a reward of -5\n",
    "grid_world = np.array([\n",
    "    ['S', '0', '0'],\n",
    "    ['0', 'X', '0'],\n",
    "    ['0', '0', 'G']\n",
    "])\n",
    "\n",
    "# Define the Q-learning parameters\n",
    "num_actions = 4  # Up, Down, Left, Right\n",
    "num_states = grid_world.size\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 100\n",
    "exploration_prob = 0.3  # Epsilon-greedy policy: 0.3 probability of exploration\n",
    "\n",
    "# Initialize the Q-table\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Convert grid-world positions to state indices\n",
    "def state_to_index(state):\n",
    "    return np.ravel_multi_index(state, dims=grid_world.shape)\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = (0, 0)  # Start at the top-left corner (the starting state)\n",
    "    state_index = state_to_index(state)\n",
    "    done = False\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Epsilon-greedy policy for action selection\n",
    "        if random.uniform(0, 1) < exploration_prob:\n",
    "            action = random.randint(0, num_actions - 1)  # Exploration\n",
    "        else:\n",
    "            action = np.argmax(q_table[state_index, :])  # Exploitation\n",
    "\n",
    "        # Take the action and observe the next state and reward\n",
    "        if action == 0:  # Up\n",
    "            next_state = (max(0, state[0] - 1), state[1])\n",
    "        elif action == 1:  # Down\n",
    "            next_state = (min(grid_world.shape[0] - 1, state[0] + 1), state[1])\n",
    "        elif action == 2:  # Left\n",
    "            next_state = (state[0], max(0, state[1] - 1))\n",
    "        else:  # Right\n",
    "            next_state = (state[0], min(grid_world.shape[1] - 1, state[1] + 1))\n",
    "\n",
    "        next_state_index = state_to_index(next_state)\n",
    "\n",
    "        # Define rewards based on the grid-world environment\n",
    "        if grid_world[next_state] == 'G':\n",
    "            reward = 10\n",
    "            done = True  # Episode terminates when reaching the goal\n",
    "        elif grid_world[next_state] == 'X':\n",
    "            reward = -5\n",
    "            done = True  # Episode terminates when hitting an obstacle\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        # Q-value update using the Bellman equation\n",
    "        q_table[state_index, action] = (1 - learning_rate) * q_table[state_index, action] + \\\n",
    "                                      learning_rate * (reward + discount_factor * np.max(q_table[next_state_index, :]))\n",
    "\n",
    "        state = next_state\n",
    "        state_index = next_state_index\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# Let's see the learned Q-table\n",
    "print(\"Learned Q-table:\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[-7.50929079 -7.49251443 -7.51855734 -7.35936254]\n",
      " [-7.50165735 -7.52832401 -7.50408279 -7.40917331]\n",
      " [-7.52190605 -7.53347346 -7.49215804 -7.36257012]\n",
      " [-7.51098399 -7.51062212 -7.45910204 -7.32761524]\n",
      " [-7.55188285 -7.49393492 -7.48395608 -7.29319942]\n",
      " [-7.61905481 -7.31816336 -7.51070413 -7.21871997]\n",
      " [-7.51967279 -7.27297657 -7.52541663 -7.4824177 ]\n",
      " [-7.42291471 -7.42944232 -7.37510252 -7.01815522]\n",
      " [-7.37091315 -7.25824988 -7.41786013 -6.98392107]\n",
      " [-7.42278636 -7.00431737 -7.36425247 -7.39036975]\n",
      " [-7.56506096 -7.58999126 -7.66360655 -7.56024587]\n",
      " [-7.56148931 -7.55830365 -7.54875374 -7.51194906]\n",
      " [-7.5323289  -7.49428488 -7.56023627 -7.5508069 ]\n",
      " [-7.53888889 -7.62374956 -7.53582397 -7.45716593]\n",
      " [-7.57695078 -7.6034445  -7.63148532 -7.51763529]\n",
      " [-7.50890145 -7.49105379 -7.52933886 -7.36737842]\n",
      " [-7.44852126 -7.20764776 -7.44717584 -7.20550465]\n",
      " [-7.09353221 -7.3344764  -7.38338214 -7.1112536 ]\n",
      " [-7.35558174 -7.17659696 -7.37244103 -7.35957925]\n",
      " [-7.36142557 -7.10017163 -7.33152859 -7.34281929]\n",
      " [-7.53092254 -7.57076009 -7.59038644 -7.54383994]\n",
      " [-7.54601355 -7.62136867 -7.55848287 -7.57595987]\n",
      " [-7.52570548 -7.54461626 -7.580399   -7.47234884]\n",
      " [-7.45448091 -7.44572877 -7.57637255 -7.52160361]\n",
      " [-7.50921548 -7.49111387 -7.48611076 -7.36657638]\n",
      " [-7.50218816 -7.45756435 -7.52684018 -7.49262485]\n",
      " [-7.43811024 -7.30962587 -7.42909485 -7.21031341]\n",
      " [-7.31483337 -7.41863385 -7.39482028 -7.37266283]\n",
      " [-7.34546764 -7.16724165 -7.26411945 -7.24790897]\n",
      " [-7.32636569 -7.18021893 -7.19367912 -7.27822556]\n",
      " [-7.51849505 -7.51422122 -7.60041811 -7.59525367]\n",
      " [-7.62776232 -7.54768765 -7.53832749 -7.57068851]\n",
      " [-7.46327304 -7.45822447 -7.46435373 -7.56149822]\n",
      " [-7.526954   -7.51601523 -7.52677982 -7.5160439 ]\n",
      " [-7.42706713 -7.44776985 -7.45742211 -7.55719556]\n",
      " [-7.4479642  -7.45642448 -7.42779889 -7.44503145]\n",
      " [-7.41114723 -7.46873387 -7.38230629 -7.36965383]\n",
      " [-7.42355742 -7.44492979 -7.37723611 -7.41059633]\n",
      " [-7.31408529 -7.05540653 -7.23106506 -7.30651254]\n",
      " [-7.22315005 -7.25060737 -7.22994453 -7.25893916]\n",
      " [-7.55463761 -7.53488125 -7.67936693 -7.63749348]\n",
      " [-7.53309739 -7.54378349 -7.56748793 -7.56898785]\n",
      " [-7.26254973 -7.42373731 -7.47018144 -7.2900223 ]\n",
      " [-7.46834587 -7.46334393 -7.51399137 -7.31409513]\n",
      " [-7.56643612 -7.48092695 -7.47499523 -7.48825282]\n",
      " [-7.51757575 -7.4918894  -7.50138402 -7.50112597]\n",
      " [-7.38786637 -7.51081781 -7.54087259 -7.47422459]\n",
      " [-7.45631831 -7.4607592  -7.45690591 -7.44831247]\n",
      " [-7.38151506 -7.14654674 -7.26651854 -7.38494645]\n",
      " [-7.49611095 -7.33277471 -7.37807532 -7.54782223]\n",
      " [-7.53512865 -7.53643207 -7.7211158  -7.61699755]\n",
      " [-7.58431022 -7.58651896 -7.5528222  -7.56414806]\n",
      " [-7.37657604 -7.51964687 -7.4043137  -7.50404868]\n",
      " [-7.48554245 -7.6012026  -7.41853544 -7.41511736]\n",
      " [-7.46178191 -7.51256753 -7.36299062 -7.38845957]\n",
      " [-7.54964772 -7.55672849 -7.39322877 -7.54620036]\n",
      " [-7.5713689  -7.52100957 -7.58048668 -7.47217198]\n",
      " [-7.45753395 -7.44394992 -7.62572062 -7.54962533]\n",
      " [-7.54478295 -7.34451596 -7.45576733 -7.46241157]\n",
      " [-7.62096057 -7.39932833 -7.45230606 -7.7306862 ]\n",
      " [-7.57824753 -7.57281646 -7.73671918 -7.6547906 ]\n",
      " [-7.57581306 -7.60572944 -7.56896222 -7.56647586]\n",
      " [-7.58565619 -7.55611499 -7.57782876 -7.56651997]\n",
      " [-7.48400968 -7.52000119 -7.50671748 -7.49680433]\n",
      " [-7.44737361 -7.52233326 -7.45779501 -7.5374174 ]\n",
      " [-7.36196895 -7.31501184 -7.35344804 -7.53201222]\n",
      " [-7.42165938 -7.60822045 -7.41086627 -7.44338187]\n",
      " [-7.60894618 -7.37370958 -7.4559046  -7.49870677]\n",
      " [-7.51865166 -7.5282702  -7.52667729 -7.55177219]\n",
      " [-7.60654347 -7.40525955 -7.5301633  -7.81309063]\n",
      " [-7.58987029 -7.52464678 -7.71762459 -7.50142141]\n",
      " [-7.54351387 -7.49861979 -7.56615467 -7.65374396]\n",
      " [-7.54313407 -7.55591508 -7.55282806 -7.5404213 ]\n",
      " [-7.48844272 -7.51008386 -7.48245055 -7.51494105]\n",
      " [-7.50873999 -7.51292264 -7.50344426 -7.51051021]\n",
      " [-7.47475719 -7.48679819 -7.46501997 -7.60447993]\n",
      " [-7.46958673 -7.4498026  -7.48431905 -7.62096116]\n",
      " [-7.5783421  -7.68445294 -7.45379685 -7.45032844]\n",
      " [-7.47116404 -7.56814789 -7.51937331 -7.46837668]\n",
      " [-7.42565701 -7.35174355 -7.66412529 -7.66615143]\n",
      " [-7.70775039 -7.67672668 -7.75176847 -7.57766986]\n",
      " [-7.57967147 -7.54764526 -7.69233708 -7.60641794]\n",
      " [-7.56739269 -7.56865032 -7.59014466 -7.57011326]\n",
      " [-7.56014619 -7.56593815 -7.64636504 -7.58033797]\n",
      " [-7.51330324 -7.51497667 -7.62774146 -7.58826429]\n",
      " [-7.5665778  -7.63565773 -7.56489477 -7.55938108]\n",
      " [-7.4661298  -7.48401223 -7.55287491 -7.37009053]\n",
      " [-7.55306301 -7.55726244 -7.58975941 -7.54875045]\n",
      " [-7.43461093 -7.6293576  -7.73038914 -7.63800658]\n",
      " [-7.53417547 -7.33622497 -7.31638514 -7.56744695]\n",
      " [-7.77219829 -7.89109256 -7.88851342 -7.78879885]\n",
      " [-7.72941004 -7.71749405 -7.63273054 -7.54741882]\n",
      " [-7.57600594 -7.67446505 -7.58711231 -7.58882644]\n",
      " [-7.59516597 -7.61832751 -7.57521071 -7.55324254]\n",
      " [-7.60322129 -7.67679426 -7.59340152 -7.58834609]\n",
      " [-7.61160777 -7.62357278 -7.61597434 -7.6111082 ]\n",
      " [-7.44538069 -7.66286401 -7.52377438 -7.53869819]\n",
      " [-7.59151287 -7.58624689 -7.43890923 -7.51543136]\n",
      " [-7.36253913 -7.56938519 -7.31187316 -7.60157785]\n",
      " [-7.52749747 -7.60843311 -7.27673514 -7.71346246]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "reward_grid = np.zeros((10,10))\n",
    "\n",
    "# Define the Q-learning parameters\n",
    "num_actions = 4  # Up, Down, Left, Right\n",
    "num_states = reward_grid.size\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 1000\n",
    "exploration_prob = 0.3  # Epsilon-greedy policy: 0.3 probability of exploration\n",
    "minstep = 1000\n",
    "\n",
    "# Initialize the Q-table\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Convert grid-world positions to state indices\n",
    "def state_to_index(state):\n",
    "    return np.ravel_multi_index(state, dims=grid_world.shape)\n",
    "\n",
    "\n",
    "def valid_actions(index,action):\n",
    "    if action == 0:\n",
    "        if index[0] == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    elif action == 1:\n",
    "        if index[0] == 9:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    elif action == 2:\n",
    "        if index[1] == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    elif action == 3:\n",
    "        if index[1] == 9:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = (0, 0)  # Start at the top-left corner (the starting state)\n",
    "    grid_world = np.zeros(reward_grid.shape)\n",
    "    grid_world[state] = 1\n",
    "    state_index = state_to_index(state)\n",
    "    done = False\n",
    "    action_list = []\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Epsilon-greedy policy for action selection\n",
    "        if random.uniform(0, 1) < exploration_prob:\n",
    "            action = random.randint(0, num_actions - 1)  # Exploration\n",
    "        else:\n",
    "            action = np.argmax(q_table[state_index, :])  # Exploitation\n",
    "        action_list.append(action)\n",
    "        if not valid_actions(state,action):\n",
    "                reward = -1\n",
    "        # Take the action and observe the next state and reward\n",
    "        if action == 0:  # Up\n",
    "            next_state = (max(0, state[0] - 1), state[1])\n",
    "        elif action == 1:  # Down\n",
    "            next_state = (min(grid_world.shape[0] - 1, state[0] + 1), state[1])\n",
    "        elif action == 2:  # Left\n",
    "            next_state = (state[0], max(0, state[1] - 1))\n",
    "        else:  # Right\n",
    "            next_state = (state[0], min(grid_world.shape[1] - 1, state[1] + 1))\n",
    "\n",
    "        next_state_index = state_to_index(next_state)\n",
    "\n",
    "        # Define rewards based on the grid-world environment\n",
    "        if grid_world[next_state] == 0:\n",
    "            reward = 1\n",
    "            grid_world[next_state] = 1\n",
    "        elif grid_world[next_state] == 1:\n",
    "            reward =  - 1\n",
    "            \n",
    "\n",
    "\n",
    "        if grid_world.all() == 1:\n",
    "            reward += 1000\n",
    "            if step < 100:\n",
    "                reward += 1000\n",
    "            if step < minstep:\n",
    "                minstep = step\n",
    "                action_list_min = action_list\n",
    "            done = True\n",
    "\n",
    "        # Q-value update using the Bellman equation\n",
    "        q_table[state_index, action] = (1 - learning_rate) * q_table[state_index, action] + \\\n",
    "                                      learning_rate * (reward + discount_factor * np.max(q_table[next_state_index, :]))\n",
    "\n",
    "        state = next_state\n",
    "        state_index = next_state_index\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# Let's see the learned Q-table\n",
    "print(\"Learned Q-table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 3\n",
      "right\n",
      "(0, 1) 3\n",
      "right\n",
      "(0, 2) 3\n",
      "right\n",
      "(0, 3) 3\n",
      "right\n",
      "(0, 4) 3\n",
      "right\n",
      "(0, 5) 3\n",
      "right\n",
      "(0, 6) 1\n",
      "down\n",
      "(1, 6) 1\n",
      "down\n",
      "(2, 6) 1\n",
      "down\n",
      "(3, 6) 3\n",
      "right\n",
      "(3, 7) 3\n",
      "right\n",
      "(3, 8) 0\n",
      "up\n",
      "(2, 8) 3\n",
      "right\n",
      "(2, 9) 3\n",
      "right\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 1 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7740/973733809.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for axis 1 with size 10"
     ]
    }
   ],
   "source": [
    "a = np.zeros((10,10))\n",
    "grid = np.zeros((10,10))\n",
    "init = (0,0)\n",
    "a[init] = 1\n",
    "step = 1\n",
    "while step < 100:\n",
    "    action = np.argmax(q_table[init[0]*3 + init[1]])\n",
    "    print(init, action)\n",
    "    if action == 0:\n",
    "        print('up')\n",
    "        init = (init[0]-1,init[1])\n",
    "    elif action == 1:\n",
    "        print('down')\n",
    "        init = (init[0]+1,init[1])\n",
    "    elif action == 2:\n",
    "        print('left')\n",
    "        init = (init[0],init[1]-1)\n",
    "    elif action == 3:\n",
    "        print('right')\n",
    "        init = (init[0],init[1]+1)\n",
    "    a[init] = 1\n",
    "    grid[init] = step\n",
    "    step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0., 98., 99.,  0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2013f9cf940>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAD8CAYAAAB6iWHJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM3ElEQVR4nO3df+hd9X3H8edrMcZZa/0Jxphph+LWdWusIbUIQ7TiD4oZzDL9o9WiZJS62rFBywaO9Z/Z/dFCsXQElWkprUVbl5UUiZjSlk1rDDHVOG0mDJPKtNFGQ1vbyHt/3BP39dvP12juuefe+H0+4PI9Pz65788l4ZXzPefc805VIUnz/c60JyBpNhkOkpoMB0lNhoOkJsNBUpPhIKlprHBIckKSTUl+0v08foFxrybZ1r02jFNT0jAyzn0OSf4ZeKGqbk7yWeD4qvpMY9y+qjpmjHlKGti44fAkcEFVPZtkOfC9qjq7Mc5wkA4z44bDz6vquG45wIsH1ueN2w9sA/YDN1fVvQu83zpgHcA7js65f3DmkYc8N6kvT20/etpTmJiXefFnVXVya98RB/vDSe4HTmns+vu5K1VVSRZKmtOraneS3wceSPLjqvrv+YOqaj2wHmD1+46qH9238mDTkybuklNXTXsKE3N/3f0/C+07aDhU1YcW2pfkf5Msn/NrxXMLvMfu7ufTSb4HnAP8VjhImh3jXsrcAFzTLV8D/Nv8AUmOT7KsWz4JOB/YMWZdSRM2bjjcDFyc5CfAh7p1kqxOcms35g+BLUkeBTYzOudgOEgz7qC/VryRqtoDXNTYvgW4vlv+D+CPx6kjaXjeISmpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDU1Es4JLk0yZNJdnadr+bvX5bkrm7/Q0nO6KOupMkZOxySLAG+DFwGvAe4Osl75g27jlHDmzOBLwKfH7eupMnq48hhDbCzqp6uql8D3wDWzhuzFrijW74buKjrkCVpRvURDiuAZ+as7+q2NcdU1X5gL3BiD7UlTchMnZBMsi7JliRbnt/z6rSnIy1qfYTDbmBuU8vTum3NMUmOAN4F7Jn/RlW1vqpWV9Xqk09c0sPUJB2qPsLhYeCsJO9OciRwFaM2eXPNbZt3JfBAjdPeW9LEjdXxCkbnEJLcANwHLAFur6rHk3wO2FJVG4DbgK8m2Qm8wChAJM2wscMBoKo2AhvnbbtpzvKvgI/0UUvSMGbqhKSk2WE4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIajIcJDUN1Svz2iTPJ9nWva7vo66kyRn7AbNzemVezKjb1cNJNlTVjnlD76qqG8atJ2kYfTx9+rVemQBJDvTKnB8Ob8lT24/mklNXjT87SYdkqF6ZAH+eZHuSu5OsbOx/XTu83/BKD1OTdKiGOiH578AZVfUnwCb+v+P268xth7eUZQNNTVLLIL0yq2pPVR04FLgVOLeHupImaJBemUmWz1m9Aniih7qSJmioXpmfSnIFsJ9Rr8xrx60rabIyq82uj80J9YFcNO1pSG9r99fdj1TV6tY+75CU1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIauqrHd7tSZ5L8tgC+5PkS127vO1J3t9HXUmT09eRw78Cl77B/suAs7rXOuArPdWVNCG9hENVfZ/RU6UXsha4s0YeBI6b97h6STNmqHMOb6plnu3wpNkxUyckbYcnzY6hwuGgLfMkzZahwmED8LHuqsV5wN6qenag2pIOwdjt8ACSfB24ADgpyS7gH4ClAFX1L8BG4HJgJ/AL4ON91JU0Ob2EQ1VdfZD9BXyyj1qShjFTJyQlzQ7DQVKT4SCpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUtNQ7fAuSLI3ybbudVMfdSVNTi/PkGTUDu8W4M43GPODqvpwT/UkTdhQ7fAkHWaGPOfwwSSPJvlukj9qDbAdnjQ7+vq14mC2AqdX1b4klwP3Muq4/TpVtR5YD3BsTqiB5iapYZAjh6p6qar2dcsbgaVJThqitqRDM0g4JDklSbrlNV3dPUPUlnRohmqHdyXwiST7gV8CV3VdsCTNqKHa4d3C6FKnpMOEd0hKajIcJDUZDpKaDAdJTYaDpCbDQVKT4SCpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNY0dDklWJtmcZEeSx5Pc2BiTJF9KsjPJ9iTvH7eupMnq4xmS+4G/qaqtSd4JPJJkU1XtmDPmMkZ9Ks4CPgB8pfspaUaNfeRQVc9W1dZu+WXgCWDFvGFrgTtr5EHguCTLx60taXJ6PeeQ5AzgHOChebtWAM/MWd/FbweI7fCkGdJbOCQ5BrgH+HRVvXQo71FV66tqdVWtXsqyvqYm6RD0Eg5JljIKhq9V1bcaQ3YDK+esn9ZtkzSj+rhaEeA24Imq+sICwzYAH+uuWpwH7K2qZ8etLWly+rhacT7wUeDHSbZ12/4O+D14rR3eRuByYCfwC+DjPdSVNEFjh0NV/RDIQcYU8Mlxa0kajndISmoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIajIcJDUN1Q7vgiR7k2zrXjeNW1fSZA3VDg/gB1X14R7qSRrAUO3wJB1m+jhyeM0btMMD+GCSR4GfAn9bVY83/vw6YB3AURzd59Rmxn0/3TbtKegtuuTUVdOewlT0Fg4HaYe3FTi9qvYluRy4l1HH7depqvXAeoBjc0L1NTdJb90g7fCq6qWq2tctbwSWJjmpj9qSJmOQdnhJTunGkWRNV3fPuLUlTc5Q7fCuBD6RZD/wS+CqrguWpBk1VDu8W4Bbxq0laTjeISmpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDU1McDZo9K8qMkj3bt8P6xMWZZkruS7EzyUNffQtIM6+PI4RXgwqp6H7AKuDTJefPGXAe8WFVnAl8EPt9DXUkT1Ec7vDrQkwJY2r3mP1l6LXBHt3w3cNGBR9VLmk19NbVZ0j2W/jlgU1XNb4e3AngGoKr2A3uBE/uoLWkyegmHqnq1qlYBpwFrkrz3UN4nybokW5Js+Q2v9DE1SYeo16sVVfVzYDNw6bxdu4GVAEmOAN5Fo+NVVa2vqtVVtXopy/qcmqS3qI+rFScnOa5b/l3gYuC/5g3bAFzTLV8JPGDHK2m29dEObzlwR5IljMLmm1X1nSSfA7ZU1QZGvTS/mmQn8AJwVQ91JU1QH+3wtgPnNLbfNGf5V8BHxq0laTjeISmpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIahqqV+a1SZ5Psq17XT9uXUmT1cfTpw/0ytyXZCnwwyTfraoH5427q6pu6KGepAH08fTpAg7WK1PSYSZ99JbpelY8ApwJfLmqPjNv/7XAPwHPA08Bf11VzzTeZx2wrls9G3hy7Mm9eScBPxuw3lD8XIefIT/b6VV1cmtHL+Hw2puNOl99G/irqnpszvYTgX1V9UqSvwT+oqou7K1wD5JsqarV055H3/xch59Z+WyD9Mqsqj1VdaAz7q3AuX3WldS/QXplJlk+Z/UK4Ilx60qarKF6ZX4qyRXAfka9Mq/toW7f1k97AhPi5zr8zMRn6/Wcg6S3D++QlNRkOEhqWvThkOTSJE8m2Znks9OeT1+S3J7kuSSPHXz04SPJyiSbk+zobte/cdpz6sOb+RrC4HNazOccupOoTzG6wrILeBi4uqp2THViPUjyp4zuXL2zqt477fn0pbvytbyqtiZ5J6Ob7/7scP87SxLgHXO/hgDc2PgawmAW+5HDGmBnVT1dVb8GvgGsnfKcelFV32d0Zehtpaqeraqt3fLLjC6Lr5jurMZXIzP1NYTFHg4rgLm3ce/ibfAPbbFIcgZwDvDQlKfSiyRLkmwDngM2VdVUP9diDwcdppIcA9wDfLqqXpr2fPpQVa9W1SrgNGBNkqn+OrjYw2E3sHLO+mndNs2w7nfye4CvVdW3pj2fvi30NYShLfZweBg4K8m7kxwJXAVsmPKc9Aa6E3e3AU9U1RemPZ++vJmvIQxtUYdDVe0HbgDuY3Ri65tV9fh0Z9WPJF8H/hM4O8muJNdNe049OR/4KHDhnCeLXT7tSfVgObA5yXZG/2ltqqrvTHNCi/pSpqSFLeojB0kLMxwkNRkOkpoMB0lNhoOkJsNBUpPhIKnp/wBOKP8K827EuQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
